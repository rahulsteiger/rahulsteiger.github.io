<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://rahulsteiger.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://rahulsteiger.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-27T22:46:56+00:00</updated><id>https://rahulsteiger.github.io/feed.xml</id><title type="html">blank</title><subtitle>A collection of my projects, publications, and blog posts. </subtitle><entry><title type="html">Biased Coin</title><link href="https://rahulsteiger.github.io/blog/2025/biased-coin/" rel="alternate" type="text/html" title="Biased Coin"/><published>2025-01-27T00:00:00+00:00</published><updated>2025-01-27T00:00:00+00:00</updated><id>https://rahulsteiger.github.io/blog/2025/biased-coin</id><content type="html" xml:base="https://rahulsteiger.github.io/blog/2025/biased-coin/"><![CDATA[<h2 id="problem-statement">Problem Statement</h2> <p>Suppose $p$ is uniform between $[0, 1]$ and you throw a coin $N$ times with bias $p$. You have to guess at each toss what the face will be. If your guess correctly, you get $1$ dollar, otherwise $-1$ dollar. What is the optimal strategy and the expected payoff in that case?</p> <h2 id="strategy">Strategy</h2> <p>Since $p$ is uniform on $[0, 1]$, it does not matter what we guess for the first flip. After a certain number of throws, our best estimate for $p$ is the number of heads divided by the current number of throws. Since we want to maximize the number of correct guess, we predict heads if we have seen more heads and tails if we saw more tails in the previous throws. If we saw an equal number of heads and tails, we just predict randomly.</p> <h2 id="a-useful-property">A useful property</h2> <p>Suppose $X_0, …, X_n$ are i.i.d uniformly distributed variables on $[0, 1]$. We define</p> \[Y_i = \begin{cases} 1 &amp; \text{if } X_i &lt; X_0 \\ 0 &amp; \text{if } X_i &gt; X_0 \end{cases}\] <p>Given $X_0$, we have that $Y_1, \ldots, Y_n$ are conditionally independent of each other. We get that</p> \[P[Y_1 + \ldots + Y_n = k | X_0] = {n \choose k} X_0^k (1 - X_0)^{n - k}\] <p>Consequently,</p> \[E[Y_1 + \ldots + Y_n = k] = \int_0^1 {n \choose k} x^k (1 - x)^{n - k} \, dx = \frac{1}{n + 1}\] <p><strong>Note:</strong> The event $Y_1 + \ldots + Y_n = k$ is equivalent to saying that if $X_0, \ldots, X_n$ are ordered, then $X_0$ has position $k+1$. This happens with probability $\frac{1}{n + 1}$.</p> <h2 id="expected-payoff">Expected Payoff</h2> <p>Let $X_n$ denote the payoff after the $n$’th toss, and $H$ denote the number of heads we saw until now. If we have $\frac{H}{n} = 0.5$, we will randomly guess, meaning that our overall expected payoff will be $0$. We have that:</p> \[\mathbb{E}[X_n | p] = \mathbb{E}\left[X_n \bigg | \frac{H}{n} &lt; 0.5, p\right] P\left[\frac{H}{n} &lt; 0.5 | p\right] + \mathbb{E}\left[X_n \bigg | \frac{H}{n} &gt; 0.5, p\right] P\left[\frac{H}{n} &gt; 0.5 | p \right]\] <p>Since we predict heads if $\frac{H}{n} &gt; 0.5$, we have that</p> \[\mathbb{E}\left[X_n \bigg | \frac{H}{n} &lt; 0.5, p\right] = (1 - p) \cdot 1 + p \cdot (-1) = 1 - 2p\] <p>Since we have that \(P\left[\frac{H}{n} &lt; 0.5 \left . \right \vert p\right] = \sum_{k=0}^{\lfloor \frac{n}{2} \rfloor} {n \choose k} p^k (1-p)^{n - k}\), we get that</p> \[\begin{align*} \int_0^{1} 2p \cdot P\left[\frac{H}{n} &lt; 0.5 | p \right] \cdot 1 dp &amp;= 2 \int_0^{1} \sum_{k=0}^{\lfloor \frac{n}{2} \rfloor} {n \choose k} p \cdot (1-p)^k p^{n - k} dp \\ &amp;= 2 \sum_{k=0}^{\lfloor \frac{n}{2} \rfloor} {n \choose k} \int_0^{1} (1-p)^{k} p^{(n + 1 - k)} dp \\ &amp;= 2 \sum_{k=0}^{\lfloor \frac{n}{2} \rfloor} {n \choose k} \frac{1}{(n + 2) \cdot {n+1 \choose k + 1}} \\ &amp;= 2 \sum_{k=0}^{\lfloor \frac{n}{2} \rfloor} \frac{k+1}{(n+1)(n+2)} \\ &amp;= \frac{(\lfloor \frac{n}{2} \rfloor + 1) (\lfloor \frac{n}{2} \rfloor + 2)}{(n + 1) (n + 2)} \end{align*}\] <p>Furthermore,</p> \[\begin{align*} \int_0^{1} 1 \cdot P\left[\frac{H}{n} &lt; 0.5 | p \right] 1 \cdot dp &amp;= \int_0^{1} \sum_{k=0}^{\lfloor \frac{n}{2} \rfloor} \cdot {n \choose k} p^k (1 - p)^{n - k} dp \\ &amp;= \sum_{k=0}^{\lfloor \frac{n}{2} \rfloor} {n \choose k} \int_0^{1} p^{k} (1 - p)^{(n - k)} dp \\ &amp;= \sum_{k=0}^{\lfloor \frac{n}{2} \rfloor} \frac{1}{(n+1)} \\ &amp;= \frac{\lfloor \frac{n}{2} \rfloor + 1}{(n + 1)} \end{align*}\] <p>Consequently,</p> \[\begin{align*} \int_0^1 E\left[Y \bigg | \frac{H}{n} &lt; 0.5, p\right] P\left[\frac{H}{n} &lt; 0.5 | p\right] \cdot 1 dp &amp;= \frac{\lfloor \frac{n}{2} \rfloor + 1}{(n + 1)} - \frac{(\lfloor \frac{n}{2} \rfloor + 1) (\lfloor \frac{n}{2} \rfloor + 2)}{(n + 1) (n + 2)} \\ &amp;= \frac{(\lfloor \frac{n}{2} \rfloor+1) (n - \lfloor \frac{n}{2} \rfloor)}{(n+1)(n+2)}\\ &amp;= \frac{(\lfloor \frac{n}{2} \rfloor+1) (\lceil \frac{n}{2} \rceil)}{(n+1)(n+2)} \end{align*}\] <p>Since the case $\frac{H}{n} &gt; 0.5$ is symmetric, we get that</p> \[E[X_n] = 2 \frac{(\lfloor \frac{n}{2} \rfloor+1) (\lceil \frac{n}{2} \rceil)}{(n+1)(n+2)}\] <p>Finally, we have that</p> \[\begin{align*} E[X] &amp;= \sum_{n=0}^{N-1} E[X_n] \\ &amp;= \sum_{n=1}^{N-1} 2 \frac{(\lfloor \frac{n}{2} \rfloor+1) (\lceil \frac{n}{2} \rceil)}{(n+1)(n+2)} \\ &amp;= 2 \sum_{n=1, n \text{ odd}}^{N-1} \frac{(n + 1)^2/2^2}{(n+1)(n+2)} + 2 \sum_{n=1, n \text{ even}}^{N-1} \frac{\frac{n + 2}{2} \frac{n}{2}}{(n+1)(n+2)} \\ &amp;= \sum_{n=1, n \text{ odd}}^{N-1} \frac{n + 1}{2 (n+2)} + \sum_{n=1, n \text{ even}}^{N-1} \frac{n}{2(n+1)} \\ &amp;= \sum_{n=2, n \text{ even}}^{N} \frac{n}{2 (n+1)} + \sum_{n=1, n \text{ even}}^{N-1} \frac{n}{2(n+1)} \\ &amp;= \frac{1}{2} \sum_{k=1}^{\lfloor \frac{N}{2} \rfloor} \left[1 - \frac{1}{2k + 1} \right] + \frac{1}{2} \sum_{k=1}^{\lfloor \frac{N-1}{2} \rfloor} \left[1 - \frac{1}{2k + 1} \right]\\ &amp;= \frac{N-1}{2} - \frac{1}{2} \sum_{k=1}^{\lfloor \frac{N}{2} \rfloor} \frac{1}{2k + 1} - \frac{1}{2} \sum_{k=1}^{\lfloor \frac{N-1}{2} \rfloor} \frac{1}{2k + 1} \end{align*}\] <p>For $N = 100$, we get that $E[X] \approx 47.55727465647559$.</p> <p><strong>Note:</strong> I do not believe that it would have been feasible to come up with the closed-form solution during an actual interview.</p> <h3 id="a-valuable-life-lesson">A Valuable Life Lesson</h3> <p>For the record, I did not receive this question during an interview; rather, I obtained it from a group chat. After coming up with what I believe is an elegant solution, I shared it with a couple of acquaintances. One of those people actually encountered this question in their interview but did not inform the interviewer. While they were unable to come up with this solution during the interview, that person sent an email to the firm submitting my solution as their own work without my permission or giving me credit. This action most likely allowed them to skip an interview stage and propelled them to the final round. It took a lot of willpower not to report that person for plagiarism. However, let me just say that I was not particularly surprised or disappointed when that person did not receive an offer.</p> <p>Happy Ending? ¯\_(ツ)_/¯</p>]]></content><author><name>Rahul Steiger</name></author><category term="coin-tosses"/><category term="probability"/><category term="quantitative-trading"/><summary type="html"><![CDATA[This blog post presents a nice analytical solution I came up with for a quantitative trading interview question and a valuable life lesson for me.]]></summary></entry><entry><title type="html">Ensemble Methods</title><link href="https://rahulsteiger.github.io/blog/2025/ensemble-methods/" rel="alternate" type="text/html" title="Ensemble Methods"/><published>2025-01-26T00:00:00+00:00</published><updated>2025-01-26T00:00:00+00:00</updated><id>https://rahulsteiger.github.io/blog/2025/ensemble-methods</id><content type="html" xml:base="https://rahulsteiger.github.io/blog/2025/ensemble-methods/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Ensemble methods combine multiple simple learning algorithms to produce an overall better result. First, we will provide a brief overview of weak and strong learners and the concept of a decision tree. In the next step, we will introduce two ensemble methods based on bagging: random forests and extremely randomized trees. The second part of the blog post will detail gradient boosting algorithms, focusing on introducing the conventional Gradient Boosting Machine and discussing the principle ideas behind the notoriously popular method XGBoost. Finally, we will conclude with a comparative study of the performance of these models on a representative sample dataset.</p> <h3 id="weak-vs-strong-classifiers">Weak vs Strong classifiers</h3> <p>A so-called weak learner or base learner is an algorithm or model that performs “slightly better than chance” <d-cite key="murphy-2012" page="555"></d-cite>. This means that the model has some but minimal predictive power. This base learner could, for example, be a shallow decision tree such as a decision tree stump explored in <a href="#decision-trees">this section</a>. A strong learner is a model that can have an arbitrarily small error in the training data set <d-cite key="murphy-2012"></d-cite>.</p> <p><strong> $\gamma$-Weak Learnability <d-cite key="shwartz-2014"></d-cite><d-cite key="princeton-weak-learnability"></d-cite></strong></p> <p>A learning problem is defined as $\gamma$-weakly learnable for a given hypothesis class $\mathcal{H}$ if for $\gamma &gt; 0$ there exists a function $m_\mathcal{H} : (\delta, \gamma) \to \mathbb{N}$ and an effective algorithm such that for any $0 &lt; \delta &lt; 1$ when the algorithm processes $m_\mathcal{H}(\delta, \gamma)$ examples drawn from a distribution $\mathcal{D}$ over the input-output space $\mathcal{X} \times \mathcal{Y}$, it produces a hypothesis $h \in \mathcal{H}$ that achieves the following error rate with probability of least $1 - \delta$:</p> <p> $$ \mathrm{error}_{\mathcal{D}}(h) \leq \frac{1}{2} - \gamma. $$ </p> <p>Strong learnability can be defined completely analogously using the error bound $\mathrm{error}_{\mathcal{D}}(h) \leq \gamma$.</p> <h4 id="combining-weak-regression-learners-to-reduce-variance">Combining weak regression learners to reduce variance</h4> <p>Consider a standard regression problem with a dataset: $\mathcal{D} = \{ (x_i, y_i) \mid i = 1, \dots, \lvert \mathcal{D} \rvert \}$. Assume we are given a set of $n$ regression estimators: $\hat{f}_1(x), \dots, \hat{f}_n(x)$. We define:</p> \[\hat{f}(x) = \frac{1}{n} \sum_{i=1}^n \hat{f}_i(x)\] <p>We can write the bias-variance decomposition of $\hat{f}$ as follows:</p> \[\begin{align*} \mathbb{E}[(\hat{f}(x) - \mathbb{E}[y \mid x])^2] &amp;= (\mathbb{E}[\hat{f}(x)] - \mathbb{E}[y \mid x])^2 + \mathbb{E}[(\hat{f}(x) - \mathbb{E}[\hat{f}(x)])^2] \\ &amp;= \operatorname{Bias}[\hat{f}(x)]^2 + \operatorname{Var}[\hat{f}(x)] \end{align*}\] <p>By the definition of $\hat{f}$, the bias term simplifies to:</p> \[\begin{align*} \operatorname{Bias}[\hat{f}(x)] &amp;= \mathbb{E}[\hat{f}(x)] - \mathbb{E}[y|x] \\ &amp;= \frac{1}{n} \sum_{i=1}^n \mathbb{E}[\hat{f}_i(x)] - \mathbb{E}[y \mid x] \\ &amp;= \frac{1}{n} \sum_{i=1}^n [\mathbb{E}[\hat{f}_i(x)] - \mathbb{E}[y \mid x]] \\ &amp;= \frac{1}{n} \sum_{i=1}^n \operatorname{Bias}[\hat{f}_i(x)] \end{align*}\] <p>By the definition of $\hat{f}$, the variance term can be rewritten as:</p> \[\begin{align*} \operatorname{Var}[\hat{f}(x)] &amp;= \mathbb{E}[(\hat{f}(x) - \mathbb{E}[\hat{f}(x)])^2] \\ &amp;= \mathbb{E}\left[\left(\frac{1}{n} \sum_{i=1}^n \hat{f}_i(x) - \frac{1}{n} \sum_{i=1}^n \mathbb{E}[\hat{f}_i(x)]\right)^2\right] \\ &amp;= \mathbb{E}\left[\left(\frac{1}{n}\sum_{i=1}^n \left(\hat{f}_i(x) - \mathbb{E}[\hat{f}_i(x)]\right)\right)^2\right] \\ &amp;= \frac{1}{n^2} \sum_{i=1}^n \sum_{j=1}^n \mathbb{E}\left[\left(\hat{f}_i(x) - \mathbb{E}[\hat{f}_i(x)]\right) \left(\hat{f}_j(x) - \mathbb{E}[\hat{f}_j(x)]\right)\right] \\ &amp;= \frac{1}{n^2} \sum_{i=1}^n \operatorname{Var}[\hat{f}_i(x)] + \frac{1}{n^2} \sum_{i \neq j} \operatorname{Cov}[\hat{f}_i(x), \hat{f}_j(x)] \end{align*}\] <p>Assuming that $f_i(x)$ and $f_j(x)$ are independent of each other:</p> \[\operatorname{Cov}[\hat{f}_i(x), \hat{f}_j(x)] = 0 \quad \text{for } i \neq j,\] <p>we will get that:</p> \[\operatorname{Var}[\hat{f}(x)] = \frac{1}{n^2} \sum_{i=1}^n \operatorname{Var}[\hat{f}_i(x)].\] <p>Consequently, we can reduce the variance arbitrarily by just using more estimators. However, assuming that $f_1, \dots, f_n$ are mutually independent is unrealistic if we assume they are trained on the same dataset. In this blog post, we will discuss algorithms that combine multiple weak learners while ensuring that $\operatorname{Cov}[\hat{f}_i(x), \hat{f}_j(x)] \approx 0$ and a sufficiently low bias.</p> <hr/> <h2 id="decision-trees">Decision Trees</h2> <p>Decision trees are a popular sequential model, relying on the recursive partition of the input space into disjoint subspaces and then training a model in each resulting region. The decision tree splits the space by applying a test to check if a specific value fulfills a certain splitting condition. Decision trees can be interpreted as a predictor $h : \mathcal{X} \to \mathcal{Y}$ where the prediction is based on the traversal of the tree from the root to a leaf. One of their main advantages is their natural interpretation. A decision tree can be used for both classification and regression purposes. However, we will focus on the former in this section. <d-cite key="murphy-2012"></d-cite> <d-cite key="shwartz-2014"></d-cite> <d-cite key="kotsiantis-2013"></d-cite></p> <h3 id="main-idea">Main Idea</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ensemble-methods/Decision_Tree_v2-480.webp 480w,/assets/img/ensemble-methods/Decision_Tree_v2-800.webp 800w,/assets/img/ensemble-methods/Decision_Tree_v2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ensemble-methods/Decision_Tree_v2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><em>Figure 1: Schematic representation of partitioning of the feature space into two disjoint subspaces based on feature ${x}_i$ and threshold $\theta$. The space for ${x}_i &lt; \theta$ is patterned in red. Each side corresponds to exactly one leaf.</em></p> <p>Following the discussion in <d-cite key="shwartz-2014"></d-cite>, one way to split the data is by applying a threshold $\theta$ to a given numeric feature dimension. We can check if the $i$’th feature of $\mathbf{x}$ is smaller than the threshold: $x_i &lt; \theta$. Therefore, we move to the left child if $\mathbb{1}_{[\mathbf{x}_i &lt; \theta]}$ as illustrated in Figure 1. This then divides our $d$-dimensional space $\mathcal{X}=\mathbb{R}^d$ into two parts with a direct correspondence between the leaves and subspaces. In general, the decision tree divides the feature space into axis-aligned hyperplanes <d-cite key="rokach-2016"></d-cite>. It is also possible to follow a similar procedure for categorical values and check them against a set of values <d-cite key="kotsiantis-2013"></d-cite>.</p> <h3 id="growth-stage">Growth Stage</h3> <p><strong>Algorithm ID3</strong> for ${-1, 1}^d \to {-1, 1}$, <em>adapted from <d-cite key="shwartz-2014"></d-cite>, <d-cite key="kaewrod-2018" fig="1"></d-cite></em> \(\begin{array}{ll} \textbf{Input:} &amp; \text{Training set } S \subseteq \mathcal{D} = \{(\mathbf{x}_i, y_i) \mid \mathbf{x}_i \in \{-1, 1\}^d, y_i \in \{-1, 1\}\}_{i=1}^n, \\ &amp; \text{Subset of features } F \subseteq \{1, \ldots, d\} \\ \textbf{Output:} &amp; \text{Decision tree} \end{array}\)</p> \[\begin{array}{l} \textbf{function} \texttt{ID3}(S, F) \\ \quad \textbf{if } F = \emptyset \textbf{ or all } y_i \text{ in } S \text{ are the same} \\ \quad \quad \textbf{return } \operatorname{Leaf}(\operatorname{mode}(Y)) \text{ with } Y = \{y_1, \ldots, y_n\} \\ \quad \textbf{end if} \\ \quad \text{Select feature } j \text{ maximizing information gain: } j = \underset{i \in F}{\arg \max} \operatorname{InfoGain}(S, i) \\ \quad \text{Partition } S \text{ into } S_{+1} = \{(\mathbf{x}, y) \in S \mid x_j = +1\} \text{ and } S_{-1} = \{(\mathbf{x}, y) \in S \mid x_j = -1\} \\ \quad \textbf{if } S_{+1} = \emptyset \textbf{ or } S_{-1} = \emptyset \\ \quad \quad \textbf{return } \operatorname{Leaf}(\operatorname{mode}(Y)) \\ \quad \textbf{else} \\ \quad \quad \text{Left branch } T_{+1} \leftarrow \texttt{ID3}(S_{+1}, F \setminus \{j\}) \\ \quad \quad \text{Right branch } T_{-1} \leftarrow \texttt{ID3}(S_{-1}, F \setminus \{j\}) \\ \quad \quad \textbf{return } \text{tree with decision node on } x_j, \text{ left branch } T_{-1}, \text{ and right branch } T_{+1} \\ \quad \textbf{end if} \\ \textbf{end function} \end{array}\] <p>Constructing an optimal decision tree—i.e., a decision tree optimal w.r.t. some gain metric—is generally an NP-hard problem and thus computationally very costly to solve. However, several greedy algorithms that are locally optimal have been developed that can solve the problem approximately but with reasonable efficiency in practical cases. <d-cite key="shwartz-2014"></d-cite> <d-cite key="kotsiantis-2013"></d-cite></p> <p>We start by assigning the root node the majority class label and then build the decision tree by iteratively finding the best split in our input data, i.e., we partition the feature space into two or more parts. Following a greedy approach, we only consider one leaf at each iteration and try to maximize some gain metric (later defined) by splitting based on one feature. After computing the gain for all possible splits, we select the split with the highest gain. A leaf is defined as pure if only one class label is associated with it; otherwise, it is considered impure. We grow the tree until the tree either consists only of pure leaves or the gain metric falls below a certain threshold. It is important to store the class labels for each leaf, as these are not necessarily pure. <d-cite key="murphy-2012"></d-cite> <d-cite key="shwartz-2014"></d-cite> <d-cite key="kotsiantis-2013"></d-cite></p> <h3 id="gain-metric-information-gain">Gain Metric: Information Gain</h3> <p>Several gain metrics are used in practice, and some metrics may lead to trees with high variance but low bias, while others might produce the opposite effect. The most commonly used metrics are information gain and Gini impurity <d-cite key="kotsiantis-2013"></d-cite>. Let us focus on the definition of the information gain.</p> <p>Information gain is a measure used to determine the effectiveness of a feature in splitting a dataset into subgroups that are more homogeneous in terms of the output variable. Consider a dataset \(\mathcal{D} = \\{(\mathbf{x}_i, y_i)\\}_{i=1}^n\) consisting of pairs of feature vectors $\mathbf{x}_i$ and their corresponding labels $y_i$. For a subset $S \subseteq \mathcal{D}$, we assess the potential of the $i$-th feature of $\mathbf{x}$, denoted as $x_i$, to split $S$ into smaller subsets. The set of all possible values $x_i$ is represented by $\operatorname{values}(i)$. For each value $v$ that $x_i$ might assume, we define $S_i(v) \subseteq S$ as the subset of $S$ where the $i$-th feature’s value is $v$: \(S_i(v) = \\{\mathbf{x} \in S \mid x_i = v\\}\).</p> <p>The conditional entropy of $S$ given the feature $x_i$, denoted as $\operatorname{H}(S \mid i)$, quantifies the entropy within $S$ after it has been partitioned according to $x_i$’s values. It is calculated as follows:</p> <p>The conditional entropy $\operatorname{H}(S \mid i)$ is defined as:</p> \[\operatorname{H}(S \mid i) = \sum_{v \in \operatorname{values}(i)} \frac{\mid S_i(v) \mid}{\mid S \mid} \cdot \operatorname{H}\left( S_i(v) \right),\] <p>where $\operatorname{H}\left( S_i(v) \right)$ represents the Shannon entropy of subset $S_i(v)$, computed using the formula:</p> \[H = - \sum_k p_k \log{p_k},\] <p>with $p_k$ denoting the relative frequency of class $k$ within the subset.</p> <p>The information gain from splitting $S$ on the feature $x_i$, denoted as $\operatorname{InfoGain}(S, i)$, is then defined as the reduction in entropy achieved by this partitioning. It is computed as the difference between the original entropy of $S$ and the weighted average entropy after the split based on $x_i$ <d-cite key="kotsiantis-2013"></d-cite> <d-cite key="myles-2004"></d-cite>:</p> \[\begin{align*} \operatorname{InfoGain}(S, i) &amp;= \operatorname{H(S)} - \operatorname{H}(S \mid i) \\ &amp;= \operatorname{H(S)} - \sum_{v \in \operatorname{values}(i)} \frac{\mid S_i(v) \mid}{\mid S \mid} \cdot \operatorname {H}\left( S_i(v) \right), \end{align*}\] <p>where $\operatorname{H(S)}$ is the Shannon entropy of the entire subset $S$ before the split. This measure serves as a criterion for selecting the feature that best divides the dataset into groups with more distinct outcomes, thereby enabling the construction of decision trees where, for every leaf, we would choose the split that maximizes the information gain or equivalently minimizes the entropy <d-cite key="murphy-2012"></d-cite>.</p> <h3 id="pruning-stage">Pruning Stage</h3> <p>The general problem for decision trees is that they tend to overfit if fully grown, resulting in very little training loss. On the contrary, if the tree is shallow, it generalizes better but will have a higher bias. One way to resolve this problem is by selectively removing parts of the tree. <d-cite key="shwartz-2014"></d-cite></p> <p>Pruning, i.e., removing parts of the tree, improves its generalization to unseen data and thus tries to prevent overfitting. We differentiate between pre- and post-pruning, where we either impose a stopping criterion while building the tree or after the tree has been fully built <d-cite key="murphy-2012"></d-cite>.</p> <p>For pre-pruning, during the growth of the decision tree, certain criteria can be imposed, e.g., the maximum depth of the tree or the minimum information gain of a split. If these criteria can no longer be met, the algorithm terminates. This is referred to as pre-pruning or early-stopping. <d-cite key="kotsiantis-2013"></d-cite></p> <p>Post-pruning allows the tree first to grow fully. It then undergoes a pruning process where nodes are evaluated for removal based on their contribution to the model’s classification accuracy on a validation set. One example is the so-called “minimal-cost complexity pruning” that involves balancing the accuracy of the tree against its complexity <d-cite key="breiman-1984"></d-cite> referenced in <d-cite key="murphy-2012"></d-cite>. In essence, minimal cost-complexity pruning is about finding a tree that is complex enough to model the underlying patterns in the data accurately but not so complex that it memorizes the training data and performs poorly on unseen data. <d-cite key="murphy-2012"></d-cite> <d-cite key="shwartz-2014"></d-cite> <d-cite key="kotsiantis-2013"></d-cite> <d-cite key="breiman-1984"></d-cite></p> <h2 id="random-forest">Random Forest</h2> <p>Deep decision trees are prone to overfitting, limiting their generalization ability to unseen data. One improvement is the implementation of Random Forests, a concept pioneered by <d-cite key="breiman-2001"></d-cite>, which is widely popular with over 58,000 citations according to Web of Science/Inspec®.</p> <p>The main idea is to grow and combine an ensemble of many (potentially shallow) decision trees with some “injected randomness” to reduce bias on unseen data <d-cite key="breiman-2001"></d-cite>. This method significantly reduces the overfitting tendency of individual trees, thereby enhancing the model’s predictive performance on new data. Generally, random forests tend to have high predictive accuracy <d-cite key="murphy-2012"></d-cite>.</p> <p><strong>Random Forest Training Algorithm</strong>, adapted from <d-cite key="shwartz-2014"></d-cite>, <d-cite key="weinberger-2018"></d-cite> \(\\ \begin{array}{l} \textbf{Input:} &amp; \text{Data Set } \mathcal{D} = \{(\mathbf{x}_i, y_i) \mid \mathbf{x}_i \in \mathbb{R}^d \}_{i=1}^n \\ \textbf{Output:} &amp; \text{Classifier } \hat{h}(\mathbf{x}) = \frac{1}{M} \sum_{j=1}^{M} h_j(\mathbf{x}) \end{array}\) \(\begin{array}{l} \textbf{Procedure: } \text{Random-Forest(S)} \\ \quad \textbf{Initialize: } \text{Ensemble of trees } \mathcal{H} = \emptyset \\ \quad \text{Sample } M \text{ datasets } \{S_j\}_{j=1}^M \text{ from } \mathcal{D} \text{ with replacement, each of size } m \leq n \\ \quad \textbf{for } j = 1 \text{ to } M \textbf{ do} \\ \quad \quad \text{Sample a subset of features } I_t \subseteq [d] \text{ of size } \mid I_t \mid = k \leq d \text{ without replacement} \\ \quad \quad \text{Train a decision tree } h_j \text{ on dataset } S_j \text{ using only features from } I_t \text{ at each split } t \\ \quad \quad \text{Add } h_j \text{ to the ensemble } \mathcal{H} \leftarrow \mathcal{H} \cup \{h_j\} \\ \quad \textbf{end for} \\ \quad \textbf{Return: } \text{Classifier } \hat{h}(\mathbf{x}) = \frac{1}{M} \sum_{j=1}^{M} h_j(\mathbf{x}) \end{array}\)</p> <p>A Random Forest builds upon a dataset $\mathcal{D} = \{ (\mathbf{x}_i, y_i) \mid \mathbf{x}_i \in \mathbb{R}^d \}_{i=1}^n$ by generating $M$ decision trees, each from a bootstrap sample $S_j$ of $S = \{\mathbf{x} \in \mathcal{D} \}$. A bootstrap sample is a multiset of $S$ created by sampling with replacement following a uniform distribution over $S$.</p> <p>In contrast to the conventional decision tree algorithm, the random forest only considers the best split among $k$ features. At each split during the tree-building process, these $k$ features are uniformly sampled without replacement from the $d$ total features. This introduces variability and enhances the model’s generalization ability. The ensemble’s prediction for a new instance is determined by combining votes across all trees. For example, this can be done by using a majority vote for classification tasks and averaging for regression tasks; a description can be found in <d-cite key="murphy-2012"></d-cite>, <d-cite key="rokach-2016"></d-cite>, <d-cite key="weinberger-2018"></d-cite>.</p> <p>It is also possible to employ early stopping or pruning for the individual trees. In particular, decision trees based on a single decision are called decision tree stumps. For the pruning step, instead of growing the tree to its full depth based on data $S_j$, we can use the left-out samples $S \setminus S_j$ as a validation set and post-prune the decision tree. <d-cite key="weinberger-2018"></d-cite></p> <p>An advantage of the random forest is that it only has two hyperparameters: the number of subsampled datasets $M$ and the number of subsampled features $k$; it is very insensitive to both <d-cite key="weinberger-2018"></d-cite>.</p> <h2 id="extremely-randomized-trees">Extremely Randomized Trees</h2> <p>Extremely randomized trees (Extra-Trees) introduce a high degree of randomization in selecting splits for both attributes and cut points. Unlike conventional methods that seek to find the optimal split based on specific criteria (such as information gain), Extra-Trees selects these splits totally or partially at random. The extreme case is that the structure of the built tree does not correlate at all with the labels of the training set. The benefit of this method is that it is computationally very efficient to implement. <d-cite key="geurts-2006"></d-cite></p> <p>Traditional tree models are significantly influenced by the randomness of the learning sample, leading to a high variance in the model outcomes. The Extra-Trees algorithm aims to leverage this randomness as a core component of the model-building process. By selecting splits randomly, the method seeks to reduce the variance associated with the choice of cut points, which has been identified as a significant contributor to the error rates in tree-based methods. In addition, this algorithm is computationally very efficient despite the necessity of growing multiple models. <d-cite key="rokach-2016"></d-cite> <d-cite key="geurts-2006"></d-cite></p> <p>The ensemble model for the Extra-Trees has two parameters: $k$, the number of features randomly selected for each node, and the early stopping criterion $n_\mathrm{min}$ that is, if the subset $S$ has strictly fewer samples than $n_\mathrm{min}$ the algorithm terminates. In addition, the algorithm does not use bootstrapped sub-samples as for the random forest but instead considers the whole input data set $M$ times. As usual, the final classification is determined using a majority vote. <d-cite key="geurts-2006"></d-cite></p> <p>The corresponding algorithm to build and ExtraTree is defined using that for a given set $S$, $\operatorname{values}_S(i) = \operatorname{values} \{ \mathbf{x} \in \mathcal{S} \mid \mathbf{x}_i \}$ is the set of all possible values of the i’th feature in $\mathcal{S}$.</p> <p><strong>Extra-Tree Training Algorithm</strong>, <em>adapted from <d-cite key="geurts-2006"></d-cite></em> \(\begin{array}{l} \textbf{Input:} &amp;\text{Training set } S \subseteq \mathcal{D} = \{(\mathbf{x}_i, y_i) \mid \mathbf{x}_i \in \mathbb{R}^d, y_i \in \{-1, 1\}\}_{i=1}^n \\ \textbf{Output:} &amp;\text{Decision tree} \end{array}\) \(\begin{array}{l} \textbf{Procedure: } \text{Extra-Trees}(S) \\ \quad \textbf{if } |S| &lt; n_\text{min} \textbf{ or all } y_i \text{ in } S \text{ are identical or all } \vec{x}_i \text{ in } S \text{ are identical} \\ \quad \quad \textbf{return } \operatorname{Leaf}(\operatorname{mode}(Y)), \text{ where } Y = \{y_1, \ldots, y_n\} \text{ and ties are broken randomly} \\ \quad \textbf{else} \\ \quad \quad \text{Sample a random subset of features } I \subseteq [d] \text{ of size } |I| = k \leq d \text{ without replacement} \\ \quad \quad \textbf{for each attribute } i \text{ in } I \textbf{ do} \\ \quad \quad \quad s_i = \text{Pick a Random Split}(S, i) \\ \quad \quad \textbf{end for} \\ \quad \quad \text{Find } s^\star \text{ such that } s^\star = \underset{i=1, \dots, k}{\arg \max} \operatorname{InfoGain}(S,s_i) \\ \quad \quad \text{Split } S \text{ into subsets } S_{\text{left}} \text{ and } S_{\text{right}} \text{ according to split } s^\star \\ \quad \quad T_{\text{left}} \leftarrow \text{Extra-Tree}(S_{\text{left}}) \\ \quad \quad T_{\text{right}} \leftarrow \text{Extra-Tree}(S_{\text{right}}) \\ \quad \quad \textbf{return } \text{tree with decision node on } s^\star, \text{ left branch } T_{\text{left}}, \text{ and right branch } T_{\text{right}} \\ \quad \textbf{end if} \end{array}\)</p> <p>\(\begin{array}{l} \textbf{Input: } &amp;\text{Whole Training Set } \mathcal{D}, \text{ Training Set } S \subseteq \mathcal{D}, \text{ Feature } i \\ \textbf{Output: } &amp; \text{Split } s_i \text{ on feature } i \\ \end{array}\) \(\begin{array}{l} \textbf{Procedure: } \text{Pick a Random Split}(S, i) \\ \quad \text{Let } \mathcal{A}_\mathcal{D} = \operatorname{values}_{\mathcal{D}}(i) \text{ be the set of all possible values for } i \text{ in } \mathcal{D} \\ \quad \text{Let } \mathcal{A}_S = \operatorname{values}_S(i) \subseteq \mathcal{A}_\mathcal{D}, \text{ i.e., the subset of } \mathcal{A}_\mathcal{D} \text{ values that appears in } S \\ \quad \text{Randomly draw a proper non-empty subset } \mathcal{A}_1 \subset \mathcal{A}_S \text{ and a subset } \mathcal{A}_2 \subset \mathcal{A}_\mathcal{D} \setminus \mathcal{A}_S \\ \quad \textbf{return } \text{split } s_i \in \mathcal{A}_1 \cup \mathcal{A}_2 \text{ on feature } i \end{array}\)</p> <h2 id="gradient-boosting">Gradient Boosting</h2> <p>The main idea behind boosting is to go from a weak classifier to a strong classifier. Given a current ensemble, one way to approach this problem is to train another classifier where misclassified samples are given a higher importance. This is known as Adaboost <d-cite key="adaboost"></d-cite>. Instead of giving misclassified samples higher importance, you can also sequentially train new classifiers to approximate the residuals of the previous ensemble. Adding them to the previous ensemble will “correct” it and is known as gradient boosting. This section presents gradient boosting based on the work in <d-cite key="gbm_orig"></d-cite>.</p> <h3 id="formal-definition">Formal Definition</h3> <p>Our goal is to find a function $\hat{F} \in \mathcal{F}$ that best approximates $y$ given $x$. A function $F \in \mathcal{F}$ is of the form for some $M \in \mathbb{N}$:</p> \[F(x) = \sum_{m=1}^M h_m(x) \gamma_m + c = \sum_{m=0}^M \gamma_m h_m(x)\] <p>For simplicity of notation, define $\gamma_0 h_0(x) = c$. We have that $\gamma_1, \dots, \gamma_n, c \in \mathbb{R}$ are constants and $h_1, …, h_m \in \mathcal{H}$ are known as base learners. For example, these base learners could be decision trees or linear regression models.</p> <p>We will define:</p> \[\begin{align} &amp;F_0(x) = c = \underset{a}{\arg \min } \mathbb{E}_{x, y} \left[\mathcal{L}(y, a)\right] \end{align}\] <p>We further define for $m \geq 1$ and some differentiable loss function $\mathcal{L}$:</p> \[\begin{align} r_m &amp;= - \nabla_{F_{m-1}} \mathcal{L}(y, F_{m-1}(x)) \label{math_res}\\ h_m &amp;= \underset{h \in \mathcal{H}}{\arg \min } \mathbb{E}_{x, y}\left[(r_m - h(x))^2\right] \label{math_h} \\ \gamma_m &amp;= \underset{\gamma}{\arg \min } \mathbb{E}_{x, y}[\mathcal{L}\left(y, F_{m-1}(x) + \gamma h_m(x)\right)] \label{math_gamma} \\ F_m(x) &amp;= F_{m-1}(x) + \gamma_m h_m(x) \label{math_F} \end{align}\] <p>In the literature, $r_m$ is generally referred to as the pseudo-residual. For a given parameter $M \in \mathbb{N}$, we will define our approximation $\hat{F}(x) = F_M(x)$.</p> <h3 id="connection-to-steepest-gradient-descent">Connection to Steepest Gradient Descent</h3> <p>Consider the following optimization problem:</p> \[\begin{align} &amp;h_m=\underset{h \in \mathcal{H}}{\arg \min } \mathbb{E}_{x, y} \left[\mathcal{L}\left(y, F_{m-1}\left(x\right)+h\left(x\right)\right)\right] \label{math_opt_orig} \end{align}\] <p>For a general function class $\mathcal{H}$, the optimization problem in equation \(\ref{math_opt_orig}\) is infeasible.</p> <p>As we have seen in the lecture, we can find a local minimum of a function with steepest gradient descent <d-cite key="gd_steepest"></d-cite>. Given a function $F_{m-1} \in \mathcal{F}$, an iteration of steepest gradient descent with the aim of minimizing $\mathcal{L}$ yields:</p> \[\begin{align} \hat{\gamma}_m = \underset{\gamma}{\arg \min } \mathbb{E}_{x, y}\left[\mathcal{L}\left(y, F_{m-1}(x) - \gamma \nabla_{F_{m-1}} \mathcal{L}(y, F_{m-1}(x)) \right) \right]\\ \hat{F}_m(x) = F_{m-1}(x) - \gamma_m \mathbb{E}_{x, y}\left[\nabla_{F_{m-1}} \mathcal{L}(y, F_{m-1}(x))\right] \end{align}\] <p>By the properties of steepest gradient descent, we have</p> \[\mathbb{E}_{x, y}\left[\mathcal{L}(y, \hat{F}_m(x))\right] \leq \mathbb{E}_{x, y}\left[\mathcal{L}(y, F_{m-1}(x))\right]\] <p>However, we do not necessarily have \(\hat{F}_m \in \mathcal{F}\) given that $F_{m-1} \in \mathcal{F}$.</p> <p>Equation \(\ref{math_h}\) defines an approximation in $\mathcal{H}$ of \(- \mathbb{E}_{x, y}\left[\nabla_{F_{m-1}} \mathcal{L}(y, F_{m-1}(x)) \right]\). Consequently, we can understand gradient boosting as an approximation of the steepest gradient descent method, where we approximate the gradient by some function $h \in \mathcal{H}$.</p> <h3 id="algorithm-for-a-finite-dataset">Algorithm for a finite Dataset</h3> <p>Consider a finite dataset $\mathcal{D} = \{ (x_i, y_i) \mid i = 1, \dots, n \} $:</p> <p><strong>Gradient Boosting</strong>, <em>adapted from <d-cite key="gbm_orig"></d-cite></em> <br/> \(\begin{array}{l} F_0(x) = \underset{a}{\arg \min } \mathbb{E}_{y}[\mathcal{L}(y, a)] \\ \textbf{for } m = 1 \text{ to } M \textbf{ do} \\ \quad r_i = - \nabla_{F_{m-1}} \mathcal{L}(y_i, F_{m-1}(x_i)) \text{ for } i=1,\dots, n \\ \quad h_m = \underset{h \in \mathcal{H}}{\arg \min } \sum_{i=1}^n (r_i - h(x_i))^2 \\ \quad \gamma_m = \underset{\gamma}{\arg \min } \sum_{i=1}^n \mathcal{L}(y_i, F_{m-1}(x_i) + \gamma h(x_i)) \\ \quad F_m(x) = F_{m-1}(x) + \gamma_m h_m(x) \\ \textbf{end for} \\ \textbf{return } F_M(x) \end{array}\)</p> <h3 id="regularization">Regularization</h3> <p>Compared to the random forest, boosting iteratively “corrects” ensemble models over the course of training. Although this might lead to higher performance on the dataset, this might hurt generalization since we might overfit the training data by “over-correcting” for it. This becomes especially problematic if our training data is noisy or even contains outliers; since then, we will be fitting the noise. Consequently, modern implementations <d-cite key="scikit-learn"></d-cite>, <d-cite key="lgbm"></d-cite>, <d-cite key="xgboost"></d-cite> have specific parameters for regularization.</p> <h4 id="regularize-base-learners">Regularize Base Learners</h4> <p>Similar to other machine learning methods, we can add a regularization parameter $\Omega$ that penalizes the complexity of the model. For example, we could adapt the definition of $h_m$ as follows:</p> \[h_m = \underset{h \in \mathcal{H}}{\arg \min } \mathbb{E}_{x, y}\left[(r - h(x))^2\right] + \Omega(h)\] <h4 id="early-stopping">Early Stopping</h4> <p>Early Stopping is a method that is used to estimate the number of weak learners required for training. Before training the model, we set aside a part of the training data, which we will refer to as the validation data. We evaluate our model on this validation set by adding new estimators during training. As soon as the performance on the validation set decreases or does not improve, we will stop training. This means that no new estimators are added.</p> <h4 id="shrinkagelearning-rate">Shrinkage/Learning Rate</h4> <p>We can add a parameter $\nu$, which scales the contribution of each weak learner. The update in equation \(\ref{math_opt_orig}\) will be changed to:</p> \[F_m(x) = F_{m-1}(x) + \nu \cdot \gamma_m h_m(x)\] <p>In literature <d-cite key="gbm_orig"></d-cite>, this parameter is called shrinkage. However, many implementations, such as the one from <code class="language-plaintext highlighter-rouge">scikit-learn</code> <d-cite key="scikit-learn"></d-cite>, call this parameter <code class="language-plaintext highlighter-rouge">learning_rate</code> instead. Similar to gradient descent, where a lower learning rate will result in slower convergence, we will need more estimators to converge. Empirical results have shown that $\nu \leq 0.1$ combined with early stopping achieves the best results <d-cite key="scikit-learn"></d-cite>.</p> <h4 id="subsampling">Subsampling</h4> <p>Stochastic Gradient Boosting <d-cite key="gbm_stochastic"></d-cite> is a minor modification to the algorithm. It consists of training each base learner only on a subsample of the training data. In <code class="language-plaintext highlighter-rouge">scikit-learn</code> <d-cite key="scikit-learn"></d-cite>, this parameter is called <code class="language-plaintext highlighter-rouge">subsample</code>. We have that $0 &lt;$ <code class="language-plaintext highlighter-rouge">subsample</code> $\leq 1$.</p> <h3 id="gradient-tree-boosting">Gradient tree boosting</h3> <p>Consider the case where we use decision trees as base learners. $h_m$ is of the following form:</p> \[h_m(x) = \sum_{j=1}^{J_m} b_{j,m} \mathbb{1}_{R_{j,m}}(x)\] <p>The decision tree splits the input space into $J_m$ distinct regions $R_{1,m},\dots,R_{J_m,m}$. Furthermore, $b_{j,m}$ denotes the value predicted in the $R_{j,m}$ region. \(\mathbb{1}_{R_{j,m}}(x)\) is the indicator value for $x \in R_{j,m}$.</p> <h4 id="improved-optimization">Improved Optimization</h4> <p>Another version <d-cite key="gbm_orig"></d-cite> alters equation \(\ref{math_gamma}\) and \(\ref{math_F}\) to:</p> \[\begin{align} &amp; \gamma_{j,m} = \underset{\gamma}{\arg \min } \mathbb{E}_{x \in R_{j,m}, y}[\mathcal{L}\left(y, F_{m-1}(x) + \gamma h_m(x)\right)] \\ &amp; F_m(x) = F_{m-1}(x) + \sum_{j=1}^{J_m} \gamma_{j,m} \mathbb{1}_{R_{j,m}}(x) \end{align}\] <p>In essence, we have a separate error correction for each region. This is guaranteed to not give a worse solution on a single iteration since the solution to the original optimization problem considers the same exact problem but with the constraint of $\gamma_{1,m} = \dots = \gamma_{J_m,m}$.</p> <h4 id="histogram-gradient-boosting">Histogram Gradient Boosting</h4> <p>By the iterative design of gradient boosting, the parallelization of the training process is much harder compared to the random forest. In the case of the random forest, we can naturally build multiple trees in parallel, whereas this is not possible in the case of boosting. This limited the use of the classical gradient boosting algorithm on large datasets since the training process took too long. Histogram Gradient Boosting can be used to increase the speed of building individual trees.</p> <p>The main bottleneck in building a decision tree is finding the optimal split. This requires us to compute a metric for all features and feature values, which requires $\mathcal{O}(\text{#features} \times n \log n)$ time since we need to sort all values of all our features.</p> <p>Although we cannot increase the sorting speed, we can approximate the optimal split by quantizing each feature. This is done by splitting the data values of each feature into bins. These bins are equal-density histograms, meaning all the intervals contain the same number of values. The quantized value of the feature will be the index of the interval. This index is chosen so that the ordinal order is preserved between the values of features in different intervals. Here is an example for the case where we have three bins, four samples, and two features <d-cite key="histogram_gbm"></d-cite>:</p> \[\begin{align*} \begin{bmatrix} 1.5 &amp; 0.0\\ 0.0 &amp; 5.5\\ 0.3 &amp; 7.0\\ 5.5 &amp; 8.5 \end{bmatrix} \quad\rightarrow\quad \begin{bmatrix} 1 &amp; 0\\ 0 &amp; 1\\ 0 &amp; 2\\ 2 &amp; 2 \end{bmatrix} \end{align*}\] <p>The best split is then computed by not considering the feature values but the histogram interval values. Since the number of intervals is a constant value and the indexes are chosen to be the values \(1, 2, \dots, \#\text{intervals}\), we are not required to sort the values in order to compute the gain. This results in a runtime of $\mathcal{O}(\text{#features} \times \text{#intervals})$.</p> <h2 id="xgboost">XGBoost</h2> <p>XGBoost is an open-source implementation of regularized Gradient Boosting Machines <d-cite key="xgboost"></d-cite>. It has been and is still being used extensively in machine learning competitions, where it is commonly part of the winning solution. However, many people use this model as a black box without properly understanding the underlying principles. While discussing all of the implementation details of XGBoost would go beyond this blog post, this section aims to give a broad overview of the main ideas behind XGBoost <d-cite key="xgboost"></d-cite>.</p> <h3 id="second-order-gradient-boosting">Second Order Gradient Boosting</h3> <p>XGBoost is based on the ideas behind gradient boosting. However, in contrast to the standard algorithm described in the previous section, XGBoost considers Gradients and Hessians.</p> <h3 id="formal-definition-1">Formal Definition</h3> <p>Similar to gradient boosting, our goal is to find a function $\hat{F} \in \mathcal{F}$ that best approximates $y$ given $x$. A function $F \in \mathcal{F}$ is of the form for some $M \in \mathbb{N}$:</p> \[F(x) = \sum_{m=1}^M h_m(x) \gamma_m + c = \sum_{m=0}^M \gamma_m h_m(x)\] <p>For simplicity of notation, define $\gamma_0 h_0(x) = c$. We have that \(\gamma_1, \dots, \gamma_n, c \in \mathbb{R}\) are constants and $h_1, \dots, h_m \in \mathcal{H}$ are known as base learners.</p> <p>We will define:</p> \[F_0(x) = c = \underset{a}{\arg \min } \mathbb{E}_{x, y} [\mathcal{L}(y, a)]\] <p>We further define for $m \geq 1$ and some twice differentiable loss function $\mathcal{L}$:</p> \[\begin{align} g &amp;= \nabla_{F_{m-1}(x)} \mathcal{L}(y, F_{m-1}(x)) \label{math_res_2}\\ H &amp;= \nabla^2_{F_{m-1}(x)} \mathcal{L}(y, F_{m-1}(x)) \label{math_res_2H}\\ h_m &amp;= \underset{h \in \mathcal{H}}{\arg \min } \mathbb{E}_{x, y}\left[\frac{H}{2} \left(\frac{g}{H} - h(x)\right)^2\right] \label{math_h_2}\\ F_m(x) &amp;= F_{m-1}(x) + h_m(x) \label{math_F_2} \end{align}\] <p>For a given parameter $M \in \mathbb{N}$, we will define our approximation $\hat{F}(x) = F_M(x)$.</p> <h3 id="a-second-order-approximation-of-mathcall">A second order approximation of $\mathcal{L}$</h3> <p>Consider the following optimization problem:</p> \[\begin{align} &amp;h_m= \underset{h \in \mathcal{H}}{\arg \min } \mathbb{E}_{x, y} \left[\mathcal{L}\left(y, F_{m-1}\left(x\right)+h\left(x\right)\right)\right] \label{math_opt_orig_2} \end{align}\] <p>For a general function class $\mathcal{H}$, the optimization problem in equation \(\ref{math_opt_orig_2}\) is infeasible. We will use the second-order Taylor approximation of the loss function.</p> \[\begin{align} &amp; \mathcal{L}\left(y, F_{m-1}\left(x\right)+h\left(x\right)\right) \approx \mathcal{L}(y, F_{m-1}(x)) + g h(x) + \frac{1}{2} H h(x)^2 \end{align}\] <p>The optimization problem for this approximation can be rewritten as follows:</p> \[\begin{align} h_m &amp;= \underset{h \in \mathcal{H}}{\arg \min } \mathbb{E}_{x, y} \left[\mathcal{L}(y, F_{m-1}(x)) + g h(x) + \frac{1}{2} H h(x)^2\right] \\ &amp;= \underset{h \in \mathcal{H}}{\arg \min } \mathbb{E}_{x, y} \left[g h(x) + \frac{1}{2} H h(x)^2 + \frac{1}{2}g^2\right] \\ &amp;= \underset{h \in \mathcal{H}}{\arg \min } \mathbb{E}_{x, y} \left[\frac{H}{2} \left(\frac{g}{H} - h(x)\right)^2 \right] \end{align}\] <p>This is exactly the same optimization problem as in equation \(\ref{math_h_2}\). It is worth mentioning that XGBoost adds regularization. This is done by adding $\Omega(h)$ to the minimization problem for $h_m$, which penalizes complexity. Furthermore, <a href="#shrinkage">Shrinkage/Learning Rate</a> from Gradient Boosting and feature subsampling from the Random Forest are used.</p> <h3 id="algorithm-for-a-finite-dataset-1">Algorithm for a finite Dataset</h3> <p><strong>Second Order Gradient Boosting</strong>, <em>Adapted from <d-cite key="xgboost"></d-cite></em></p> <p>Consider a finite dataset $\mathcal{D} = \{(x_i, y_i) \mid i = 1, \dots, n\}$: \(\begin{array}{l} F_0(x) = \underset{a}{\arg \min} \sum_{i=1}^n \mathcal{L}(y_i, a) \\ \textbf{for } m = 1 \text{ to } M \textbf{ do} \\ \quad g_i = \nabla_{F_{m-1}} \mathcal{L}(y, F_{m-1}(x)) \text{ for } i = 1, \dots, n \\ \quad H_i = \nabla^2_{F_{m-1}} \mathcal{L}(y, F_{m-1}(x)) \text{ for } i = 1, \dots, n \\ \quad h_m = \underset{h \in \mathcal{H}}{\arg \min } \sum_{i=1}^n \frac{H_i}{2}\left(\frac{g_i}{H_i} - h(x_i) \right)^2 \\ \quad F_m(x) = F_{m-1}(x) + h_m(x) \\ \textbf{end for} \\ \textbf{return } F_M(x) \end{array}\)</p> <h3 id="weighted-quantile-sketch">Weighted Quantile Sketch</h3> <p>For tree building, XGBoost uses an approximate solution that shares some similarities with <a href="#histogram-gradient-boosting">Histogram Gradient Boosting</a>. The main goal is to reduce the time needed to find the optimal split. Consider the multi-set \(\mathcal{D}_k = \{(x_{1,k}, H_1), (x_{2,k}, H_2), \ldots, (x_{n,k}, H_n)\}\), where $x_{i,k}$ is the $i$’th feature value of the $k$’th feature and $H_i$ is equation \(\ref{math_F_2}\) evaluated at point $x_i$. We will define the following rank function:</p> \[r_k(z) = \frac{1}{\sum_{(x,H) \in \mathcal{D}_k} H} \sum_{(x, H) \in \mathcal{D}_k, x &lt; z} H\] <p>Intuitively, this ranking function can be interpreted as the data points weighted by $H_i$. This happens in equation \(\ref{math_h_2}\) as well, which is nothing else but a weighted sum of the squared loss with labels $\frac{g_i}{H_i}$ and weights $H_i$.</p> <p>A candidate split for $\epsilon &gt; 0$ of the $k$’th feature is defined as the data points \(\{s_{k,1}, \dots, s_{k,l}\}\), such that</p> \[|r_k(s_{k,j}) - r_k(s_{k,j+1})| &lt; \epsilon, \quad s_{k,1} = \min_i x_{i,k}, \quad s_{k,l} = \max_i x_{i, k}\] <p>Finding such a candidate split is called the weighted quantile sketch problem, which is non-trivial for large datasets. The authors of <d-cite key="xgboost"></d-cite> propose an algorithm to approximate the problem. This algorithm has some nice theoretical properties. The proofs and description of the algorithm can be found in the appendix of <d-cite key="xgboost"></d-cite>.</p> <h3 id="sparsity-awareness">Sparsity Awareness</h3> <p>Many real-world datasets are sparse, which is generally caused by missing values in the data, frequent zero values in statistics, and artifacts from feature engineering (e.g., one-hot encoding).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ensemble-methods/xgboost_sparsity-480.webp 480w,/assets/img/ensemble-methods/xgboost_sparsity-800.webp 800w,/assets/img/ensemble-methods/xgboost_sparsity-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ensemble-methods/xgboost_sparsity.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><em>Figure: Handling Missing Values. Reproduced from <d-cite key="xgboost"></d-cite></em></p> <p>XGBoost adds a default direction for missing values in the decision tree. In order to generate a new split for a feature, only the non-missing values are considered. However, the split is computed for both cases where the missing values either all go to the left or the right direction. The optimal split and the default direction for the missing values are chosen based on the maximum gain. For certain very sparse datasets, this can lead to a 50x improvement in runtime <d-cite key="xgboost"></d-cite> compared to a basic solution such as imputation.</p> <h3 id="system-design">System Design</h3> <p>Here are some features of XGBoost with regard to System Design. For more details, see <d-cite key="xgboost"></d-cite>.</p> <ul> <li>Parallelization</li> <li>Cache-aware Computation, which results in a 2x improvement in runtime for very large datasets <d-cite key="xgboost"></d-cite></li> <li>Distributed Training on a Cluster</li> <li>Support for processing large datasets that do not fit onto the main disk</li> </ul> <h2 id="experiment">Experiment</h2> <p>In this section, we present the results of Random Forest, Extra Trees, Gradient Boosting, and XGBoost on real-world data. We use the implementations of <code class="language-plaintext highlighter-rouge">scikit-learn</code> <d-cite key="scikit-learn"></d-cite> and <code class="language-plaintext highlighter-rouge">xgboost</code> <d-cite key="xgboost"></d-cite> for our experiments; the corresponding implementation can be found in <a href="#code-for-experiment">Code for Experiment</a>. This section demonstrates the practical ease of use of these algorithms. This often leads to them being used as a black box.</p> <h3 id="california-housing">California Housing</h3> <p>We use the <code class="language-plaintext highlighter-rouge">California Housing</code> dataset from <code class="language-plaintext highlighter-rouge">scikit-learn</code> <d-cite key="scikit-learn"></d-cite>. This dataset consists of only eight numeric features and is a regression task consisting of predicting a house price given these features. We perform 3-fold cross-validation on the dataset and measure runtime &amp; <code class="language-plaintext highlighter-rouge">R2 Score</code>. Furthermore, we train multiple instances of each classifier, where we modify the maximum number of estimators each method uses. The error bars denote the difference in runtime between different folds.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ensemble-methods/xgboost_experiment-480.webp 480w,/assets/img/ensemble-methods/xgboost_experiment-800.webp 800w,/assets/img/ensemble-methods/xgboost_experiment-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ensemble-methods/xgboost_experiment.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><em>Figure: Runtime and R2 Score with different number of trees. The left figure presents an overview of all evaluated models, whereas the right excludes the Gradient Boosting model to enhance readability.</em></p> <p>As one can clearly see, the runtime of standard gradient boosting is significantly longer compared to the other methods. This can be explained by the fact that the iterative tree building of gradient boosting is hard to parallelize. Since the experiments were run on a machine with 128 cores, this significantly impacts runtime. Another observation that can be made is that the Random Forest and Extra Trees both plateau much earlier than the gradient methods while having the shortest runtime.</p> <h2 id="appendix">Appendix</h2> <h3 id="code-for-experiment">Code for Experiment</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># This code has been adapted from: 
# https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_hist_grad_boosting_comparison.html
</span>
<span class="kn">from</span> <span class="n">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_california_housing</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="nf">fetch_california_housing</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">as_frame</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">The dataset consists of </span><span class="si">{</span><span class="n">n_samples</span><span class="si">}</span><span class="s"> samples and </span><span class="si">{</span><span class="n">n_features</span><span class="si">}</span><span class="s"> features</span><span class="sh">"</span><span class="p">)</span>

<span class="kn">import</span> <span class="n">joblib</span>

<span class="n">N_CORES</span> <span class="o">=</span> <span class="mi">128</span><span class="c1">#joblib.cpu_count(only_physical_cores=True)
</span><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Number of physical cores: </span><span class="si">{</span><span class="n">N_CORES</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="kn">from</span> <span class="n">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>
<span class="kn">from</span> <span class="n">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">HistGradientBoostingRegressor</span><span class="p">,</span> <span class="n">RandomForestRegressor</span><span class="p">,</span> <span class="n">GradientBoostingRegressor</span><span class="p">,</span> <span class="n">ExtraTreesRegressor</span>
<span class="kn">from</span> <span class="n">xgboost</span> <span class="kn">import</span> <span class="n">XGBRegressor</span>
<span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span><span class="p">,</span> <span class="n">KFold</span>

<span class="n">models</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">Extra Trees</span><span class="sh">"</span><span class="p">:</span> <span class="nc">ExtraTreesRegressor</span><span class="p">(</span>
        <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="n">N_CORES</span><span class="p">,</span> <span class="c1">#max_depth=8, min_samples_leaf=5, min_samples_split=3, min_weight_fraction_leaf=0.1, 
</span>    <span class="p">),</span>
    <span class="c1">#"Decision Tree" : DecisionTreeRegressor(
</span>    <span class="c1">#   random_state=0
</span>    <span class="c1">#),
</span>    <span class="sh">"</span><span class="s">XGBoost</span><span class="sh">"</span> <span class="p">:</span> <span class="nc">XGBRegressor</span><span class="p">(</span>
        <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="sh">"</span><span class="s">Gradient Boosting</span><span class="sh">"</span> <span class="p">:</span> <span class="nc">GradientBoostingRegressor</span><span class="p">(</span>
        <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">min_weight_fraction_leaf</span><span class="o">=</span><span class="mf">0.1</span>
    <span class="p">),</span>
    <span class="sh">"</span><span class="s">Random Forest</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RandomForestRegressor</span><span class="p">(</span>
        <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="n">N_CORES</span><span class="p">,</span> <span class="c1">#max_depth=10, min_samples_leaf=2, min_samples_split=5, min_weight_fraction_leaf=0.1
</span>    <span class="p">),</span>

    <span class="c1">#"Hist Gradient Boosting": HistGradientBoostingRegressor(
</span>    <span class="c1">#    max_leaf_nodes=15, random_state=0, early_stopping=False
</span>    <span class="c1">#),
</span><span class="p">}</span>
<span class="n">n_estimators</span> <span class="o">=</span> <span class="p">[</span><span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">250</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">350</span><span class="p">,</span> <span class="mi">400</span><span class="p">,</span> <span class="mi">450</span><span class="p">,</span> <span class="mi">500</span><span class="p">]</span>
<span class="n">max_depth</span> <span class="o">=</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">]</span>
<span class="n">min_samples_split</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]</span>
<span class="n">min_samples_leaf</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">]</span>
<span class="n">min_weight_fraction_leaf</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]</span>

<span class="n">param_grids</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">Extra Trees</span><span class="sh">"</span> <span class="p">:</span> <span class="p">{</span>
        <span class="sh">"</span><span class="s">n_estimators</span><span class="sh">"</span><span class="p">:</span> <span class="n">n_estimators</span><span class="p">,</span>
        <span class="c1">#'max_depth' : max_depth, 
</span>        <span class="c1">#'min_samples_split' : min_samples_split, 
</span>        <span class="c1">#'min_samples_leaf' : min_samples_leaf, 
</span>        <span class="c1">#'min_weight_fraction_leaf' : min_weight_fraction_leaf
</span>    <span class="p">},</span>
    <span class="sh">"</span><span class="s">XGBoost</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span>
        <span class="sh">"</span><span class="s">n_estimators</span><span class="sh">"</span><span class="p">:</span> <span class="n">n_estimators</span><span class="p">,</span>
        <span class="c1">#'max_depth' : max_depth, 
</span>        <span class="c1">#'learning_rate' : learning_rate, 
</span>        <span class="c1">#'min_samples_split' : min_samples_split, 
</span>        <span class="c1">#'min_samples_leaf' : min_samples_leaf, 
</span>        <span class="c1">#'min_weight_fraction_leaf' : min_weight_fraction_leaf
</span>    <span class="p">},</span>
    <span class="sh">"</span><span class="s">Gradient Boosting</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span>
        <span class="sh">"</span><span class="s">n_estimators</span><span class="sh">"</span><span class="p">:</span> <span class="n">n_estimators</span><span class="p">,</span> 
        <span class="c1">#'max_depth' : max_depth, 
</span>        <span class="c1">#'learning_rate' : learning_rate, 
</span>        <span class="c1">#'min_samples_split' : min_samples_split, 
</span>        <span class="c1">#'min_samples_leaf' : min_samples_leaf, 
</span>        <span class="c1">#'min_weight_fraction_leaf' : min_weight_fraction_leaf
</span>    <span class="p">},</span>
    <span class="sh">"</span><span class="s">Random Forest</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span>
        <span class="sh">"</span><span class="s">n_estimators</span><span class="sh">"</span><span class="p">:</span> <span class="n">n_estimators</span><span class="p">,</span>
        <span class="c1">#'max_depth' : max_depth, 
</span>        <span class="c1">#'min_samples_split' : min_samples_split, 
</span>        <span class="c1">#'min_samples_leaf' : min_samples_leaf, 
</span>        <span class="c1">#'min_weight_fraction_leaf' : min_weight_fraction_leaf
</span>    <span class="p">},</span>
    <span class="sh">"</span><span class="s">Hist Gradient Boosting</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span>
        <span class="sh">"</span><span class="s">max_iter</span><span class="sh">"</span><span class="p">:</span> <span class="n">n_estimators</span>
    <span class="p">},</span>
<span class="p">}</span>

<span class="n">cv</span> <span class="o">=</span> <span class="nc">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">models</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
    <span class="n">grid_search</span> <span class="o">=</span> <span class="nc">GridSearchCV</span><span class="p">(</span>
        <span class="n">estimator</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
        <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grids</span><span class="p">[</span><span class="n">name</span><span class="p">],</span>
        <span class="n">return_train_score</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span>
        <span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span> <span class="k">if</span> <span class="n">name</span> <span class="o">!=</span> <span class="sh">"</span><span class="s">Gradient Boosting</span><span class="sh">"</span> <span class="k">else</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="c1"># train multiple models if gradient boosting since it will use at most 1 core anyway. For the other methods, only train 1 model at a time in order to use all cores for training a single model
</span>    <span class="p">).</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">model</span><span class="sh">"</span><span class="p">:</span> <span class="n">name</span><span class="p">,</span> <span class="sh">"</span><span class="s">cv_results</span><span class="sh">"</span><span class="p">:</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">grid_search</span><span class="p">.</span><span class="n">cv_results_</span><span class="p">)}</span>
    <span class="n">results</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>

    <span class="kn">import</span> <span class="n">plotly.colors</span> <span class="k">as</span> <span class="n">colors</span>
<span class="kn">import</span> <span class="n">plotly.express</span> <span class="k">as</span> <span class="n">px</span>
<span class="kn">from</span> <span class="n">plotly.subplots</span> <span class="kn">import</span> <span class="n">make_subplots</span>

<span class="n">fig</span> <span class="o">=</span> <span class="nf">make_subplots</span><span class="p">(</span>
    <span class="n">rows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">cols</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">subplot_titles</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">Train time vs score</span><span class="sh">"</span><span class="p">],</span>
    <span class="n">vertical_spacing</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">row_heights</span><span class="o">=</span><span class="p">[</span><span class="mf">5.0</span><span class="p">]</span>
<span class="p">)</span>

<span class="n">model_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">result</span><span class="p">[</span><span class="sh">"</span><span class="s">model</span><span class="sh">"</span><span class="p">]</span> <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">results</span><span class="p">]</span>
<span class="n">colors_list</span> <span class="o">=</span> <span class="n">colors</span><span class="p">.</span><span class="n">qualitative</span><span class="p">.</span><span class="n">Plotly</span> <span class="o">*</span> <span class="p">(</span>
    <span class="nf">len</span><span class="p">(</span><span class="n">model_names</span><span class="p">)</span> <span class="o">//</span> <span class="nf">len</span><span class="p">(</span><span class="n">colors</span><span class="p">.</span><span class="n">qualitative</span><span class="p">.</span><span class="n">Plotly</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
<span class="p">)</span>

<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">result</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">results</span><span class="p">):</span>
    <span class="n">cv_results</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="sh">"</span><span class="s">cv_results</span><span class="sh">"</span><span class="p">].</span><span class="nf">round</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">model_name</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="sh">"</span><span class="s">model</span><span class="sh">"</span><span class="p">]</span>
    <span class="n">param_name</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">param_grids</span><span class="p">[</span><span class="n">model_name</span><span class="p">].</span><span class="nf">keys</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">cv_results</span><span class="p">[</span><span class="n">param_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">cv_results</span><span class="p">[</span><span class="sh">"</span><span class="s">param_</span><span class="sh">"</span> <span class="o">+</span> <span class="n">param_name</span><span class="p">]</span>
    <span class="n">cv_results</span><span class="p">[</span><span class="sh">"</span><span class="s">model</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">model_name</span>

    <span class="n">scatter_fig</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span>
        <span class="n">cv_results</span><span class="p">,</span>
        <span class="n">x</span><span class="o">=</span><span class="sh">"</span><span class="s">mean_fit_time</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">y</span><span class="o">=</span><span class="sh">"</span><span class="s">mean_test_score</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">error_x</span><span class="o">=</span><span class="sh">"</span><span class="s">std_fit_time</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">error_y</span><span class="o">=</span><span class="sh">"</span><span class="s">std_test_score</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">hover_data</span><span class="o">=</span><span class="n">param_name</span><span class="p">,</span>
        <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">model</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">line_fig</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="nf">line</span><span class="p">(</span>
        <span class="n">cv_results</span><span class="p">,</span>
        <span class="n">x</span><span class="o">=</span><span class="sh">"</span><span class="s">mean_fit_time</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">y</span><span class="o">=</span><span class="sh">"</span><span class="s">mean_test_score</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">scatter_trace</span> <span class="o">=</span> <span class="n">scatter_fig</span><span class="p">[</span><span class="sh">"</span><span class="s">data</span><span class="sh">"</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">line_trace</span> <span class="o">=</span> <span class="n">line_fig</span><span class="p">[</span><span class="sh">"</span><span class="s">data</span><span class="sh">"</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">scatter_trace</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">marker</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="n">colors_list</span><span class="p">[</span><span class="n">idx</span><span class="p">]))</span>
    <span class="n">line_trace</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">line</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="n">colors_list</span><span class="p">[</span><span class="n">idx</span><span class="p">]))</span>
    <span class="n">fig</span><span class="p">.</span><span class="nf">add_trace</span><span class="p">(</span><span class="n">scatter_trace</span><span class="p">,</span> <span class="n">row</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">fig</span><span class="p">.</span><span class="nf">add_trace</span><span class="p">(</span><span class="n">line_trace</span><span class="p">,</span> <span class="n">row</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">fig</span><span class="p">.</span><span class="nf">update_layout</span><span class="p">(</span>
    <span class="n">xaxis</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="sh">"</span><span class="s">Train time (s) - lower is better</span><span class="sh">"</span><span class="p">),</span>
    <span class="n">yaxis</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="sh">"</span><span class="s">Test R2 score - higher is better</span><span class="sh">"</span><span class="p">),</span>
    <span class="n">legend</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mf">0.72</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">traceorder</span><span class="o">=</span><span class="sh">"</span><span class="s">normal</span><span class="sh">"</span><span class="p">,</span> <span class="n">borderwidth</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">title</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">text</span><span class="o">=</span><span class="sh">"</span><span class="s">Speed-score trade-off</span><span class="sh">"</span><span class="p">),</span>
    <span class="n">height</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">width</span><span class="o">=</span><span class="mi">800</span>
<span class="p">)</span>

<span class="n">fig</span><span class="p">.</span><span class="nf">write_image</span><span class="p">(</span><span class="sh">"</span><span class="s">train_time_vs_score.svg</span><span class="sh">"</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">write_image</span><span class="p">(</span><span class="sh">"</span><span class="s">train_time_vs_score.png</span><span class="sh">"</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>


<span class="kn">import</span> <span class="n">plotly.colors</span> <span class="k">as</span> <span class="n">colors</span>
<span class="kn">import</span> <span class="n">plotly.express</span> <span class="k">as</span> <span class="n">px</span>
<span class="kn">from</span> <span class="n">plotly.subplots</span> <span class="kn">import</span> <span class="n">make_subplots</span>

<span class="n">fig</span> <span class="o">=</span> <span class="nf">make_subplots</span><span class="p">(</span>
    <span class="n">rows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">cols</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">subplot_titles</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">Train time vs score</span><span class="sh">"</span><span class="p">],</span>
    <span class="n">vertical_spacing</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">row_heights</span><span class="o">=</span><span class="p">[</span><span class="mf">5.0</span><span class="p">]</span>
<span class="p">)</span>

<span class="n">model_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">result</span><span class="p">[</span><span class="sh">"</span><span class="s">model</span><span class="sh">"</span><span class="p">]</span> <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">results</span><span class="p">]</span>
<span class="n">colors_list</span> <span class="o">=</span> <span class="n">colors</span><span class="p">.</span><span class="n">qualitative</span><span class="p">.</span><span class="n">Plotly</span> <span class="o">*</span> <span class="p">(</span>
    <span class="nf">len</span><span class="p">(</span><span class="n">model_names</span><span class="p">)</span> <span class="o">//</span> <span class="nf">len</span><span class="p">(</span><span class="n">colors</span><span class="p">.</span><span class="n">qualitative</span><span class="p">.</span><span class="n">Plotly</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
<span class="p">)</span>

<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">result</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">results</span><span class="p">):</span>

    <span class="n">cv_results</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="sh">"</span><span class="s">cv_results</span><span class="sh">"</span><span class="p">].</span><span class="nf">round</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">model_name</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="sh">"</span><span class="s">model</span><span class="sh">"</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">model_name</span> <span class="o">==</span> <span class="sh">'</span><span class="s">Gradient Boosting</span><span class="sh">'</span><span class="p">:</span> <span class="k">continue</span>
        
    <span class="n">param_name</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">param_grids</span><span class="p">[</span><span class="n">model_name</span><span class="p">].</span><span class="nf">keys</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">cv_results</span><span class="p">[</span><span class="n">param_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">cv_results</span><span class="p">[</span><span class="sh">"</span><span class="s">param_</span><span class="sh">"</span> <span class="o">+</span> <span class="n">param_name</span><span class="p">]</span>
    <span class="n">cv_results</span><span class="p">[</span><span class="sh">"</span><span class="s">model</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">model_name</span>

    <span class="n">scatter_fig</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span>
        <span class="n">cv_results</span><span class="p">,</span>
        <span class="n">x</span><span class="o">=</span><span class="sh">"</span><span class="s">mean_fit_time</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">y</span><span class="o">=</span><span class="sh">"</span><span class="s">mean_test_score</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">error_x</span><span class="o">=</span><span class="sh">"</span><span class="s">std_fit_time</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">error_y</span><span class="o">=</span><span class="sh">"</span><span class="s">std_test_score</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">hover_data</span><span class="o">=</span><span class="n">param_name</span><span class="p">,</span>
        <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">model</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">line_fig</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="nf">line</span><span class="p">(</span>
        <span class="n">cv_results</span><span class="p">,</span>
        <span class="n">x</span><span class="o">=</span><span class="sh">"</span><span class="s">mean_fit_time</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">y</span><span class="o">=</span><span class="sh">"</span><span class="s">mean_test_score</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">scatter_trace</span> <span class="o">=</span> <span class="n">scatter_fig</span><span class="p">[</span><span class="sh">"</span><span class="s">data</span><span class="sh">"</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">line_trace</span> <span class="o">=</span> <span class="n">line_fig</span><span class="p">[</span><span class="sh">"</span><span class="s">data</span><span class="sh">"</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">scatter_trace</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">marker</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="n">colors_list</span><span class="p">[</span><span class="n">idx</span><span class="p">]))</span>
    <span class="n">line_trace</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">line</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="n">colors_list</span><span class="p">[</span><span class="n">idx</span><span class="p">]))</span>
    <span class="n">fig</span><span class="p">.</span><span class="nf">add_trace</span><span class="p">(</span><span class="n">scatter_trace</span><span class="p">,</span> <span class="n">row</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">fig</span><span class="p">.</span><span class="nf">add_trace</span><span class="p">(</span><span class="n">line_trace</span><span class="p">,</span> <span class="n">row</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">fig</span><span class="p">.</span><span class="nf">update_layout</span><span class="p">(</span>
    <span class="n">xaxis</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="sh">"</span><span class="s">Train time (s) - lower is better</span><span class="sh">"</span><span class="p">),</span>
    <span class="n">yaxis</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="sh">"</span><span class="s">Test R2 score - higher is better</span><span class="sh">"</span><span class="p">),</span>
    <span class="n">legend</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mf">0.72</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">traceorder</span><span class="o">=</span><span class="sh">"</span><span class="s">normal</span><span class="sh">"</span><span class="p">,</span> <span class="n">borderwidth</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">title</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">text</span><span class="o">=</span><span class="sh">"</span><span class="s">Speed-score trade-off</span><span class="sh">"</span><span class="p">),</span>
    <span class="n">height</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">width</span><span class="o">=</span><span class="mi">800</span>
<span class="p">)</span>

<span class="n">fig</span><span class="p">.</span><span class="nf">write_image</span><span class="p">(</span><span class="sh">"</span><span class="s">train_time_vs_score_no_gb.svg</span><span class="sh">"</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">write_image</span><span class="p">(</span><span class="sh">"</span><span class="s">train_time_vs_score_no_gb.png</span><span class="sh">"</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>]]></content><author><name>Julian Santen</name></author><category term="Decision-Trees"/><category term="Random-Forest"/><category term="Boosting"/><category term="XGBoost"/><summary type="html"><![CDATA[Ensemble methods combine multiple simple learning algorithms to achieve superior overall performance. This blog post is an adaptation of a group project from the CS4270 course I took at the National University of Singapore during my exchange.]]></summary></entry></feed>