<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Running DeepSeek R1 locally | Rahul Steiger </title> <meta name="author" content="Rahul Steiger"> <meta name="description" content="This blog post presents the steps required to run inference for DeepSeek R1 using llama.cpp on a single HPC node equipped with 4 A100 GPUs and 1 TB of memory."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://rahulsteiger.github.io/blog/2025/llamacpp-deepseek/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/leaflet@1.9.4/dist/leaflet.min.css" integrity="sha256-q9ba7o845pMPFU+zcAll8rv+gC+fSovKsOoNQ6cynuQ=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/styles/github.min.css" integrity="sha256-Oppd74ucMR5a5Dq96FxjEzGF7tTw2fZ/6ksAqDCM8GY=" crossorigin="anonymous" media="screen and (prefers-color-scheme: light)"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/styles/github-dark.min.css" integrity="sha256-nyCNAiECsdDHrr/s2OQsp5l9XeY2ZJ0rMepjCT2AkBk=" crossorigin="anonymous" media="screen and (prefers-color-scheme: dark)"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/diff2html@3.4.47/bundles/css/diff2html.min.css" integrity="sha256-IMBK4VNZp0ivwefSn51bswdsrhk0HoMTLc2GqFHFBXg=" crossorigin="anonymous"> <link defer rel="stylesheet" type="text/css" href="https://tikzjax.com/v1/fonts.css"> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Running DeepSeek R1 locally",
            "description": "This blog post presents the steps required to run inference for DeepSeek R1 using llama.cpp on a single HPC node equipped with 4 A100 GPUs and 1 TB of memory.",
            "published": "February 10, 2025",
            "authors": [
              
              {
                "author": "Rahul Steiger",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "ETH Zurich",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Rahul</span> Steiger </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/"> </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Running DeepSeek R1 locally</h1> <p>This blog post presents the steps required to run inference for DeepSeek R1 using llama.cpp on a single HPC node equipped with 4 A100 GPUs and 1 TB of memory.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#llama-cpp-setup">llama.cpp setup</a> </div> <div> <a href="#downloading-the-model-from-hugging-face">Downloading the model from Hugging Face</a> </div> <div> <a href="#expected-payoff">Expected Payoff</a> </div> <div> <a href="#running-the-model">Running the model</a> </div> </nav> </d-contents> <p>Running these LLMs requires significant computational resources. Fortunately, I have access to HPC hardware thanks to my previous participation in Team RACKlette during my bachelor’s. The node I am using for this experiment has 2 <code class="language-plaintext highlighter-rouge">AMD EPYC 7773X 64-Core</code> CPUs, 4 <code class="language-plaintext highlighter-rouge">NVIDIA A100 80GB</code> GPUs, and <code class="language-plaintext highlighter-rouge">1 TB</code> of memory. This is the only reason I am able to play around with LLMs.</p> <h2 id="llamacpp-setup">llama.cpp setup</h2> <p>The main goal of llama.cpp is to enable LLM inference with minimal setup and state-of-the-art performance on a wide range of hardware. More information can be found <a href="https://github.com/ggerganov/llama.cpp" rel="external nofollow noopener" target="_blank">here</a>.</p> <p>Clone the github repository:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone git@github.com:ggerganov/llama.cpp.git <span class="o">&amp;&amp;</span> <span class="nb">cd </span>llama.cpp
</code></pre></div></div> <p>Build llama.cpp:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cmake <span class="nt">-B</span> build <span class="nt">-DGGML_NATIVE</span><span class="o">=</span>OFF <span class="nt">-DGGML_CUDA</span><span class="o">=</span>ON <span class="nt">-DLLAMA_CURL</span><span class="o">=</span>ON <span class="nt">-DCMAKE_EXE_LINKER_FLAGS</span><span class="o">=</span><span class="nt">-Wl</span>,--allow-shlib-undefined <span class="nb">.</span>
cmake <span class="nt">--build</span> build <span class="nt">--config</span> Release <span class="nt">-j</span>
</code></pre></div></div> <p><strong>Note:</strong> Make sure that the cuda compiler is available (you can check it with <code class="language-plaintext highlighter-rouge">nvcc --version</code>). More information on how to install CUDA can be found <a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/" rel="external nofollow noopener" target="_blank">here</a>.</p> <h2 id="downloading-the-model-from-hugging-face">Downloading the model from Hugging Face</h2> <p>As an initial step, we will be running a model with 1.58 quantization since it has the lowest hardware requirements.</p> <p>Install the Hugging Face CLI:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install</span> <span class="nt">-U</span> <span class="s2">"huggingface_hub[cli,hf_transfer]"</span>
</code></pre></div></div> <p>Download the GGUF model files from the unsloth/DeepSeek-R1-GGUF repository:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">MODEL</span><span class="o">=</span><span class="s2">"unsloth/DeepSeek-R1-GGUF"</span>

<span class="nb">export </span><span class="nv">FILES</span><span class="o">=(</span>
  <span class="s2">"DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf"</span>
  <span class="s2">"DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00002-of-00003.gguf"</span>
  <span class="s2">"DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00003-of-00003.gguf"</span>
<span class="o">)</span>

<span class="nb">export </span><span class="nv">LOCAL_DIR</span><span class="o">=</span><span class="s2">"</span><span class="nv">$HOME</span><span class="s2">/models"</span>

<span class="nv">HF_HUB_ENABLE_HF_TRANSFER</span><span class="o">=</span>1 huggingface-cli download <span class="se">\</span>
  <span class="nt">--repo-type</span> <span class="s2">"model"</span> <span class="se">\</span>
  <span class="nt">--local-dir</span> <span class="s2">"</span><span class="nv">$LOCAL_DIR</span><span class="s2">"</span> <span class="se">\</span>
  <span class="nv">$MODEL</span> <span class="s2">"</span><span class="k">${</span><span class="nv">FILES</span><span class="p">[@]</span><span class="k">}</span><span class="s2">"</span>
</code></pre></div></div> <h2 id="running-the-model">Running the model</h2> <p>In the same terminal session as before, we can go to the go to the directory that contains the llama-server binary:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd </span>build/bin
</code></pre></div></div> <p>We can then start the inference server with the following command:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>./llama-server <span class="se">\</span>
  <span class="nt">--port</span> 8192 <span class="se">\</span>
  <span class="nt">--model</span> <span class="s2">"</span><span class="nv">$LOCAL_DIR</span><span class="s2">/</span><span class="k">${</span><span class="nv">FILES</span><span class="p">[0]</span><span class="k">}</span><span class="s2">"</span> <span class="se">\</span>
  <span class="nt">--n-gpu-layers</span> 256 <span class="se">\</span>
  <span class="nt">--tensor-split</span> 24,25,25,25 <span class="se">\</span>
  <span class="nt">--split-mode</span> row <span class="se">\</span>
  <span class="nt">--flash-attn</span> <span class="se">\</span>
  <span class="nt">--ctx-size</span> 16384
</code></pre></div></div> <p>Parameter explanation:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">--model "$LOCAL_DIR/${FILES[0]}"</code>: Path to the first GGUF file.</li> <li> <code class="language-plaintext highlighter-rouge">--tensor-split 24,25,25,25</code>: Fraction of the model that is split across the GPUs (keep a slightly lower fraction for GPU 0 for metadata).</li> <li> <code class="language-plaintext highlighter-rouge">--n-gpu-layers 256</code>: Number of layers that are offloaded to the GPU (VRAM) for acceleration.</li> <li> <code class="language-plaintext highlighter-rouge">--split-mode row</code>: The tensors are split row-wise across GPUs.</li> <li> <code class="language-plaintext highlighter-rouge">--flash-attn</code>: Use <a href="https://github.com/Dao-AILab/flash-attention" rel="external nofollow noopener" target="_blank">FlashAttention</a>.</li> <li> <code class="language-plaintext highlighter-rouge">--ctx-size 16384</code>: Number of tokens the model can process in a single context window.</li> </ul> <p><strong>Note:</strong> Further information can be found <a href="https://github.com/ggerganov/llama.cpp/blob/master/examples/server/README.md" rel="external nofollow noopener" target="_blank">here</a>.</p> <p>We can test the model with the following command:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">--request</span> POST <span class="se">\</span>
     <span class="nt">--url</span> http://localhost:8192/completion <span class="se">\</span>
     <span class="nt">--header</span> <span class="s2">"Content-Type: application/json"</span> <span class="se">\</span>
     <span class="nt">--data</span> <span class="s1">'{"prompt": "Why is the answer 42?"}'</span>
</code></pre></div></div> <p>The model gave me the following answer:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">(</span>Number Theory<span class="o">)</span> <span class="c">#2 - Mathematics Stack Exmost recent 30 from math.stackexchange.com2024Doubtful2024-07-17T14:19:43Zhttps://math.stackexchange.com/feeds/question/4921410https://creativecommons.org/licenses/by-sa/4.0/rdfhttps://math.stackexchange.com/q/49214100Why is the answer 42? (Number Theory) #2Misha Parishhttps://math.stackexchange.com/users/13142522024-05-23T16:32:05Z2024Doubtful2024-05Doubtfulhttps://math.stackexchange.com/q/49214100Why is the answer 42? (Number Theory) #2Misha Parishhttps://math.stackexchange.com/users/13142522024-05-23T16:32:05Z2024-05Doubtful&lt;p&gt;So, I'm a student, and I'm learning math from the &lt;a href="https://math.stackexchange.com/questions/4921410/why-is-the-answer-42-number-theory-2"&gt;Ground Up&lt;/a&gt; series. I have a question from &lt;a href="https://math.stackexchange.com/questions/4921410/why-is-the-answer-42-number-theory-2"&gt;Volume 1&lt;/a&gt;, and I need help. The problem is: "If a four-digit number is made by combining the numbers 1, 2, 3, and 4. What is the sum of all the possible four-digit numbers that are formed?" The answer is given as 42, but I can't figure out why. I need an explanation. Here are my thoughts:&lt;/p&gt; &lt;p&gt;I think that since there are 4 unique digits, there are 4! = 24 possible permutations. Each digit (1, 2, 3, 4) will appear in each place (thousands, hundreds, tens, ones) 6 times. So, for each digit, it appears 6 times in each position. The total sum for each position would be 6*(1+2+3+4) = 6*10 = 60. Then, for the total sum, it's 60*1000 + 60*100 + 60*10 + 60*1 = 60*(1000+100+10+1) = 60*1111 = 66660. But the answer is supposed to be 42. Where did I go wrong?</span>
</code></pre></div></div> <p>So the answer is not particularly useful.</p> <h2 id="using-a-larger-model">Using a larger model</h2> <p>We change <code class="language-plaintext highlighter-rouge">FILES</code> environment variable in <a href="#downloading-the-model-from-hugging-face">download</a> section.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">FILES</span><span class="o">=(</span>
  <span class="s2">"DeepSeek-R1-Q8_0/DeepSeek-R1.Q8_0-00001-of-00015.gguf"</span>
  <span class="s2">"DeepSeek-R1-Q8_0/DeepSeek-R1.Q8_0-00002-of-00015.gguf"</span>
  <span class="s2">"DeepSeek-R1-Q8_0/DeepSeek-R1.Q8_0-00003-of-00015.gguf"</span>
  <span class="s2">"DeepSeek-R1-Q8_0/DeepSeek-R1.Q8_0-00004-of-00015.gguf"</span>
  <span class="s2">"DeepSeek-R1-Q8_0/DeepSeek-R1.Q8_0-00005-of-00015.gguf"</span>
  <span class="s2">"DeepSeek-R1-Q8_0/DeepSeek-R1.Q8_0-00006-of-00015.gguf"</span>
  <span class="s2">"DeepSeek-R1-Q8_0/DeepSeek-R1.Q8_0-00007-of-00015.gguf"</span>
  <span class="s2">"DeepSeek-R1-Q8_0/DeepSeek-R1.Q8_0-00008-of-00015.gguf"</span>
  <span class="s2">"DeepSeek-R1-Q8_0/DeepSeek-R1.Q8_0-00009-of-00015.gguf"</span>
  <span class="s2">"DeepSeek-R1-Q8_0/DeepSeek-R1.Q8_0-00010-of-00015.gguf"</span>
  <span class="s2">"DeepSeek-R1-Q8_0/DeepSeek-R1.Q8_0-00011-of-00015.gguf"</span>
  <span class="s2">"DeepSeek-R1-Q8_0/DeepSeek-R1.Q8_0-00012-of-00015.gguf"</span>
  <span class="s2">"DeepSeek-R1-Q8_0/DeepSeek-R1.Q8_0-00013-of-00015.gguf"</span>
  <span class="s2">"DeepSeek-R1-Q8_0/DeepSeek-R1.Q8_0-00014-of-00015.gguf"</span>
  <span class="s2">"DeepSeek-R1-Q8_0/DeepSeek-R1.Q8_0-00015-of-00015.gguf"</span>
<span class="o">)</span>

<span class="sb">```</span>bash
./llama-server <span class="se">\</span>
  <span class="nt">--port</span> 8192 <span class="se">\</span>
  <span class="nt">--model</span> <span class="s2">"</span><span class="nv">$LOCAL_DIR</span><span class="s2">/</span><span class="k">${</span><span class="nv">FILES</span><span class="p">[0]</span><span class="k">}</span><span class="s2">"</span> <span class="se">\</span>
  <span class="nt">--n-gpu-layers</span> 20 <span class="se">\</span>
  <span class="nt">--tensor-split</span> 24,25,25,25 <span class="se">\</span>
  <span class="nt">--split-mode</span> row <span class="se">\</span>
  <span class="nt">--flash-attn</span> <span class="se">\</span>
  <span class="nt">--ctx-size</span> 16384
</code></pre></div></div> <p>Running the same prompt as before, we get roughly of 2.0 tokens per second.</p> <h2 id="geht-es-besser">Geht es besser?</h2> <p>At this point, our model does not fit into VRAM, meaning that we will have to use CPU for some part of the inference. Consequently, we are bound by the performance of the CPU. As I learnt in my first semester lectures, you should always ask yourself: geht es besser?</p> <p>One could potentially increase the CPU performance if we compiled llama.cpp differently. We will be using the <a href="https://github.com/flame/blis/tree/master" rel="external nofollow noopener" target="_blank">BLIS</a> libary to see if we can accelerate linear algebra operations on the CPU.</p> <p>Create a folder <code class="language-plaintext highlighter-rouge">dependencies</code> in the home directory and go into it:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">mkdir </span>dependencies <span class="o">&amp;&amp;</span> <span class="nb">cd </span>dependencies
</code></pre></div></div> <p>We then clone the BLIS repository and go into it:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone git@github.com:amd/blis.git <span class="o">&amp;&amp;</span> <span class="nb">cd </span>blis
</code></pre></div></div> <p>We configure and build BLIS as follows:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>./configure <span class="nt">--prefix</span><span class="o">=</span><span class="nb">.</span> <span class="nt">--enable-cblas</span> <span class="nt">-t</span> pthreads zen3 <span class="o">&amp;&amp;</span> make <span class="nt">-j</span> <span class="o">&amp;&amp;</span> make <span class="nb">install</span>
</code></pre></div></div> <p>We export the following variables and rebuild llama.cpp in a different folder with some additional flags:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd</span> ~/llama.cpp

<span class="nb">export </span><span class="nv">BLIS_PREFIX</span><span class="o">=</span><span class="s2">"</span><span class="nv">$HOME</span><span class="s2">/dependencies/blis"</span>
<span class="nb">export </span><span class="nv">BLAS_LIBRARIES</span><span class="o">=</span><span class="s2">"</span><span class="nv">$BLIS_PREFIX</span><span class="s2">/lib/zen3/libblis-mt.so"</span>
<span class="nb">export </span><span class="nv">BLAS_INCLUDE_DIRS</span><span class="o">=</span><span class="s2">"</span><span class="nv">$BLIS_PREFIX</span><span class="s2">/include/zen3"</span>
<span class="nb">export </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="s2">"</span><span class="nv">$BLIS_PREFIX</span><span class="s2">/lib/zen3:</span><span class="nv">$LD_LIBRARY_PATH</span><span class="s2">"</span>

cmake <span class="nt">-B</span> blis_build <span class="nt">-DGGML_BLAS</span><span class="o">=</span>ON <span class="nt">-DGGML_BLAS_VENDOR</span><span class="o">=</span>FLAME <span class="nt">-DBLAS_LIBRARIES</span><span class="o">=</span><span class="nv">$BLAS_LIBRARIES</span> <span class="nt">-DBLAS_INCLUDE_DIRS</span><span class="o">=</span><span class="nv">$BLAS_INCLUDE_DIRS</span> <span class="nt">-DGGML_NATIVE</span><span class="o">=</span>OFF <span class="nt">-DGGML_CUDA</span><span class="o">=</span>ON <span class="nt">-DLLAMA_CURL</span><span class="o">=</span>ON <span class="nt">-DCMAKE_EXE_LINKER_FLAGS</span><span class="o">=</span><span class="nt">-Wl</span>,--allow-shlib-undefined <span class="nb">.</span>
cmake <span class="nt">--build</span> blis_build <span class="nt">--config</span> Release <span class="nt">-j</span> 
</code></pre></div></div> <p>We can run llama.cpp as before with:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd </span>blis_build/bin

./llama-server <span class="se">\</span>
  <span class="nt">--port</span> 8192 <span class="se">\</span>
  <span class="nt">--model</span> <span class="s2">"</span><span class="nv">$LOCAL_DIR</span><span class="s2">/</span><span class="k">${</span><span class="nv">FILES</span><span class="p">[0]</span><span class="k">}</span><span class="s2">"</span> <span class="se">\</span>
  <span class="nt">--n-gpu-layers</span> 20 <span class="se">\</span>
  <span class="nt">--tensor-split</span> 24,25,25,25 <span class="se">\</span>
  <span class="nt">--split-mode</span> row <span class="se">\</span>
  <span class="nt">--flash-attn</span> <span class="se">\</span>
  <span class="nt">--ctx-size</span> 16384
</code></pre></div></div> <p>Running the same prompt as before, we get roughly 2.0 tokens per second as well. I did not do proper benchmarking, but from my estimate, there does not seem to be a significant difference compared to the version without BLIS. After reading more into this and scouring GitHub for answers, I concluded that if raw inference speed is what you care about, you should probably look into other frameworks such as <a href="https://github.com/NVIDIA/TensorRT" rel="external nofollow noopener" target="_blank">TensorRT</a>. However, I still find the minimal setup and decent performance of llama.cpp to be what makes it great for experimentation.</p> <h2 id="larger">Larger?</h2> <p>TODO: Run 16 Bit precision model (that does not fit into memory)</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Rahul Steiger. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/diff2html@3.4.47/bundles/js/diff2html-ui.min.js" integrity="sha256-eU2TVHX633T1o/bTQp6iIJByYJEtZThhF9bKz/DcbbY=" crossorigin="anonymous"></script> <script defer src="/assets/js/diff2html-setup.js?80a6e52ce727518bbd3aed2bb6ba5601" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/leaflet@1.9.4/dist/leaflet.min.js" integrity="sha256-MgH13bFTTNqsnuEoqNPBLDaqxjGH+lCpqrukmXc8Ppg=" crossorigin="anonymous"></script> <script defer src="/assets/js/leaflet-setup.js?b6313931e203b924523e2d8b75fe8874" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js" integrity="sha256-0q+JdOlScWOHcunpUk21uab1jW7C1deBQARHtKMcaB4=" crossorigin="anonymous"></script> <script defer src="/assets/js/chartjs-setup.js?183c5859923724fb1cb3c67593848e71" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/echarts@5.5.0/dist/echarts.min.js" integrity="sha256-QvgynZibb2U53SsVu98NggJXYqwRL7tg3FeyfXvPOUY=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/echarts@5.5.0/theme/dark-fresh-cut.js" integrity="sha256-sm6Ui9w41++ZCWmIWDLC18a6ki72FQpWDiYTDxEPXwU=" crossorigin="anonymous"></script> <script defer src="/assets/js/echarts-setup.js?738178999630746a8d0cfc261fc47c2c" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega@5.27.0/build/vega.min.js" integrity="sha256-Yot/cfgMMMpFwkp/5azR20Tfkt24PFqQ6IQS+80HIZs=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega-lite@5.16.3/build/vega-lite.min.js" integrity="sha256-TvBvIS5jUN4BSy009usRjNzjI1qRrHPYv7xVLJyjUyw=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega-embed@6.24.0/build/vega-embed.min.js" integrity="sha256-FPCJ9JYCC9AZSpvC/t/wHBX7ybueZhIqOMjpWqfl3DU=" crossorigin="anonymous"></script> <script defer src="/assets/js/vega-setup.js?7c7bee055efe9312afc861b128fe5f36" type="text/javascript"></script> <script defer src="https://tikzjax.com/v1/tikzjax.js" integrity="sha256-+1qyucCXRZJrCg3lm3KxRt/7WXaYhBid4/1XJRHGB1E=" crossorigin="anonymous"></script> <script src="/assets/js/typograms.js?062e75bede72543443762dc3fe36c7a5"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>